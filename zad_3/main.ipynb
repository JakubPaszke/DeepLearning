{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "25064b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train/train.tsv', sep='\\t', names=['label', 'text'])\n",
    "dev_x = pd.read_csv('dev-0/in.tsv', sep='\\t', names=['text'])\n",
    "dev_y = pd.read_csv('dev-0/expected.tsv', sep='\\t', names=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0949f04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.', '</S>', 'Peter', 'Blackburn', '</S>', 'BRUSSELS', '1996-08-22', '</S>', 'The', 'European', 'Commission', 'said', 'on', 'Thursday', 'it', 'disagreed', 'with', 'German', 'advice', 'to', 'consumers', 'to', 'shun', 'British', 'lamb', 'until', 'scientists', 'determine', 'whether', 'mad', 'cow', 'disease', 'can', 'be', 'transmitted', 'to', 'sheep', '.', '</S>', 'Germany', \"'s\", 'representative', 'to', 'the', 'European', 'Union', \"'s\", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.', '</S>', '\"', 'We', 'do', \"n't\", 'support', 'any', 'such', 'recommendation', 'because', 'we', 'do', \"n't\", 'see', 'any', 'grounds', 'for', 'it', ',', '\"', 'the', 'Commission', \"'s\", 'chief', 'spokesman', 'Nikolaus', 'van', 'der', 'Pas', 'told', 'a', 'news', 'briefing', '.', '</S>', 'He', 'said', 'further', 'scientific', 'study', 'was', 'required', 'and', 'if', 'it', 'was', 'found', 'that', 'action', 'was', 'needed', 'it', 'should', 'be', 'taken', 'by', 'the', 'European', 'Union', '.', '</S>', 'He', 'said', 'a', 'proposal', 'last', 'month', 'by', 'EU', 'Farm', 'Commissioner', 'Franz', 'Fischler', 'to', 'ban', 'sheep', 'brains', ',', 'spleens', 'and', 'spinal', 'cords', 'from', 'the', 'human', 'and', 'animal', 'food', 'chains', 'was', 'a', 'highly', 'specific', 'and', 'precautionary', 'move', 'to', 'protect', 'human', 'health', '.', '</S>', 'Fischler', 'proposed', 'EU-wide', 'measures', 'after', 'reports', 'from', 'Britain', 'and', 'France', 'that', 'under', 'laboratory', 'conditions', 'sheep', 'could', 'contract', 'Bovine', 'Spongiform', 'Encephalopathy', '(', 'BSE', ')', '--', 'mad', 'cow', 'disease', '.', '</S>', 'But', 'Fischler', 'agreed', 'to', 'review', 'his', 'proposal', 'after', 'the', 'EU', \"'s\", 'standing', 'veterinary', 'committee', ',', 'mational', 'animal', 'health', 'officials', ',', 'questioned', 'if', 'such', 'action', 'was', 'justified', 'as', 'there', 'was', 'only', 'a', 'slight', 'risk', 'to', 'human', 'health', '.', '</S>', 'Spanish', 'Farm', 'Minister', 'Loyola', 'de', 'Palacio', 'had', 'earlier', 'accused', 'Fischler', 'at', 'an', 'EU', 'farm', 'ministers', \"'\", 'meeting', 'of', 'causing', 'unjustified', 'alarm', 'through', '\"', 'dangerous', 'generalisation', '.', '\"', '</S>', '.', '</S>', 'Only', 'France', 'and', 'Britain', 'backed', 'Fischler', \"'s\", 'proposal', '.', '</S>', 'The', 'EU', \"'s\", 'scientific', 'veterinary', 'and', 'multidisciplinary', 'committees', 'are', 'due', 'to', 're-examine', 'the', 'issue', 'early', 'next', 'month', 'and', 'make', 'recommendations', 'to', 'the', 'senior', 'veterinary', 'officials', '.', '</S>', 'Sheep', 'have', 'long', 'been', 'known', 'to', 'contract', 'scrapie', ',', 'a', 'brain-wasting', 'disease', 'similar', 'to', 'BSE', 'which', 'is', 'believed', 'to', 'have', 'been', 'transferred', 'to', 'cattle', 'through', 'feed', 'containing', 'animal', 'waste', '.', '</S>', 'British', 'farmers', 'denied', 'on', 'Thursday', 'there', 'was', 'any', 'danger', 'to', 'human', 'health', 'from', 'their', 'sheep', ',', 'but', 'expressed', 'concern', 'that', 'German', 'government', 'advice', 'to', 'consumers', 'to', 'avoid', 'British', 'lamb', 'might', 'influence', 'consumers', 'across', 'Europe', '.', '</S>', '\"', 'What', 'we', 'have', 'to', 'be', 'extremely', 'careful', 'of', 'is', 'how', 'other', 'countries', 'are', 'going', 'to', 'take', 'Germany', \"'s\", 'lead', ',', '\"', 'Welsh', 'National', 'Farmers', \"'\", 'Union', '(', 'NFU', ')', 'chairman', 'John', 'Lloyd', 'Jones', 'said', 'on', 'BBC', 'radio', '.', '</S>', 'Bonn', 'has', 'led', 'efforts', 'to', 'protect', 'public', 'health', 'after', 'consumer', 'confidence', 'collapsed', 'in', 'March', 'after', 'a', 'British', 'report', 'suggested', 'humans', 'could', 'contract', 'an', 'illness', 'similar', 'to', 'mad', 'cow', 'disease', 'by', 'eating', 'contaminated', 'beef', '.', '</S>', 'Germany', 'imported', '47,600', 'sheep', 'from', 'Britain', 'last', 'year', ',', 'nearly', 'half', 'of', 'total', 'imports', '.', '</S>', 'It', 'brought', 'in', '4,275', 'tonnes', 'of', 'British', 'mutton', ',', 'some', '10', 'percent', 'of', 'overall', 'imports', '.', '</S>']\n",
      "labels: ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "train['tokens'] = train['text'].apply(lambda x: x.split())\n",
    "train['labels'] = train['label'].apply(lambda x: x.split())\n",
    "\n",
    "dev_x['tokens'] = dev_x['text'].apply(lambda x: x.split())\n",
    "dev_y['labels'] = dev_y['label'].apply(lambda x: x.split())\n",
    "\n",
    "print(\"text:\", train['tokens'].iloc[0])\n",
    "print(\"labels:\", train['labels'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3bf64795",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, labels in zip(train['tokens'], train['labels']):\n",
    "    if len(text) != len(labels):\n",
    "        print(f\"Text and labels length mismatch: {len(text)} vs {len(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7e4ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_to_ix(tokens_list):\n",
    "    word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for tokens in tokens_list:\n",
    "        for token in tokens:\n",
    "            if token not in word_to_ix:\n",
    "                word_to_ix[token] = len(word_to_ix)\n",
    "    return word_to_ix\n",
    "\n",
    "def build_tag_to_ix(labels_list):\n",
    "    tag_to_ix = {\"<PAD>\": 0}\n",
    "    for labels in labels_list:\n",
    "        for label in labels:\n",
    "            if label not in tag_to_ix:\n",
    "                tag_to_ix[label] = len(tag_to_ix)\n",
    "    return tag_to_ix\n",
    "\n",
    "word_to_ix = build_word_to_ix(train['tokens'])\n",
    "tag_to_ix = build_tag_to_ix(train['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "73fb7bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word to index len: 23626\n",
      "Tag to index len: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Word to index len:\", len(word_to_ix))\n",
    "print(\"Tag to index len:\", len(tag_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88a6153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    word_tensor = []\n",
    "    for word in seq:\n",
    "        word_tensor.append(to_ix.get(word, 1))\n",
    "    return word_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5179898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def pad_sequences(tensors_list):\n",
    "    padded_tensor = []\n",
    "    mask_tensor = []\n",
    "    \n",
    "    longest = len(max(tensors_list, key=len))\n",
    "\n",
    "    for tensor in tensors_list:\n",
    "        tensor = copy.deepcopy(tensor)\n",
    "        mask = [1 for number in tensor]\n",
    "        while len(tensor) != longest:\n",
    "            tensor.append(0)\n",
    "            mask.append(0)\n",
    "        padded_tensor.append(tensor)\n",
    "        mask_tensor.append(mask)\n",
    "\n",
    "    return torch.tensor(padded_tensor), torch.tensor(mask_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ace0444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=256):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "    \n",
    "    def forward(self, input_tensor):\n",
    "        embeds = self.emb(input_tensor)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = torch.log_softmax(tag_space, dim=2)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f608ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, tokens_list, labels_list, word_to_ix, tag_to_ix):\n",
    "        self.tokens_list = tokens_list\n",
    "        self.labels_list = labels_list\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokens_list[idx]\n",
    "        labels = self.labels_list[idx]\n",
    "\n",
    "        token_ids = [self.word_to_ix.get(tok, self.word_to_ix[\"<UNK>\"]) for tok in tokens]\n",
    "        label_ids = [self.tag_to_ix[label] for label in labels]\n",
    "\n",
    "        return token_ids, label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ea12ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    token_batch, label_batch = zip(*batch)\n",
    "\n",
    "    input_tensor, input_mask = pad_sequences(token_batch)\n",
    "    label_tensor, _ = pad_sequences(label_batch)\n",
    "\n",
    "    return input_tensor, label_tensor, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "144c2e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = NERDataset(train['tokens'], train['labels'], word_to_ix, tag_to_ix)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "dev_dataset = NERDataset(dev_x['tokens'], dev_y['labels'], word_to_ix, tag_to_ix)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "52861fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(len(word_to_ix), len(tag_to_ix))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.NLLLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "89a77c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "from collections import Counter\n",
    "\n",
    "def evaluation():\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []  \n",
    "    all_labels = []  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_tensor, label_tensor, input_mask in dev_loader:\n",
    "            output = model(input_tensor)  \n",
    "            preds = torch.argmax(output, dim=2)  \n",
    "\n",
    "\n",
    "            for i in range(input_tensor.size(0)): \n",
    "                pred_seq = []\n",
    "                label_seq = []\n",
    "                for j in range(input_tensor.size(1)): \n",
    "                    if input_mask[i][j] == 0:\n",
    "                        continue  # pomiń <PAD>\n",
    "                    pred_tag = list(tag_to_ix.keys())[list(tag_to_ix.values()).index(preds[i][j].item())]\n",
    "                    true_tag = list(tag_to_ix.keys())[list(tag_to_ix.values()).index(label_tensor[i][j].item())]\n",
    "                    pred_seq.append(pred_tag)\n",
    "                    label_seq.append(true_tag)\n",
    "\n",
    "                all_preds.append(pred_seq)\n",
    "                all_labels.append(label_seq)            \n",
    "    print(\"F1 score: \", f1_score(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b5d82134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 25,  Loss: 1.1544\n",
      "Epoch 2 / 25,  Loss: 0.8228\n",
      "Epoch 3 / 25,  Loss: 0.5083\n",
      "Epoch 4 / 25,  Loss: 0.3103\n",
      "Epoch 5 / 25,  Loss: 0.1856\n",
      "F1 score:  0.4423713511886849\n",
      "Epoch 6 / 25,  Loss: 0.1156\n",
      "Epoch 7 / 25,  Loss: 0.0735\n",
      "Epoch 8 / 25,  Loss: 0.0498\n",
      "Epoch 9 / 25,  Loss: 0.0370\n",
      "Epoch 10 / 25,  Loss: 0.0277\n",
      "F1 score:  0.5837044367474847\n",
      "Epoch 11 / 25,  Loss: 0.0172\n",
      "Epoch 12 / 25,  Loss: 0.0123\n",
      "Epoch 13 / 25,  Loss: 0.0079\n",
      "Epoch 14 / 25,  Loss: 0.0060\n",
      "Epoch 15 / 25,  Loss: 0.0050\n",
      "F1 score:  0.6313031048995473\n",
      "Epoch 16 / 25,  Loss: 0.0043\n",
      "Epoch 17 / 25,  Loss: 0.0043\n",
      "Epoch 18 / 25,  Loss: 0.0033\n",
      "Epoch 19 / 25,  Loss: 0.0029\n",
      "Epoch 20 / 25,  Loss: 0.0024\n",
      "F1 score:  0.6200197313500796\n",
      "Epoch 21 / 25,  Loss: 0.0021\n",
      "Epoch 22 / 25,  Loss: 0.0019\n",
      "Epoch 23 / 25,  Loss: 0.0022\n",
      "Epoch 24 / 25,  Loss: 0.0016\n",
      "Epoch 25 / 25,  Loss: 0.0015\n",
      "F1 score:  0.638929932343106\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 25\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for input_tensor, label_tensor, input_mask in loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input_tensor)\n",
    "\n",
    "        output = output.view(-1, len(tag_to_ix))         \n",
    "        label_tensor = label_tensor.view(-1)              \n",
    "\n",
    "        loss = loss_fn(output, label_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} / {num_epoch},  Loss: {loss.item():.4f}\")\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
